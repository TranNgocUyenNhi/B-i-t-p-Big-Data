{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dtap63bYWEz0"},"source":["# CS246 - Homework 1\n","\n","## Question 1\n","\n","### Spark"]},{"cell_type":"markdown","metadata":{"id":"p0-YhEpP_Ds-"},"source":["### Setup"]},{"cell_type":"markdown","metadata":{"id":"Zsj5WYpR9QId"},"source":["Let's setup Spark on your Colab environment.  Run the cell below!"]},{"cell_type":"code","metadata":{"id":"tmZp-9R-Vz16"},"source":["!pip install pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-CJ71AKe91eh"},"source":["Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.\n","\n","**Make sure to follow the interactive instructions.**"]},{"cell_type":"code","metadata":{"id":"o78Q_846WUlk"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a6z-lZAQXHdV"},"source":["Get the data"]},{"cell_type":"code","metadata":{"id":"JfTW-hKfWrpv"},"source":["id='1DWp2HRuus1Ge9vvxHYLbHzyrhb_huGag'\n","downloaded = drive.CreateFile({'id': id})\n","downloaded.GetContentFile('soc-LiveJournal1Adj.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hm9hq1vf1pkw"},"source":["Import Spark"]},{"cell_type":"code","metadata":{"id":"NFSTBSlzXPge"},"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","import pandas as pd\n","import itertools\n","import re\n","import sys\n","\n","# create the Spark Session\n","spark = SparkSession.builder.getOrCreate()\n","\n","# create the Spark Context\n","sc = spark.sparkContext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8BasEyYC2Fzf"},"source":["spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","dataset = sc.textFile(\"/content/soc-LiveJournal1Adj.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''Hàm line2dataset nhận vào một chuỗi đại diện cho một cặp (src, dst_list),\n","tách và trả về src (kiểu int) và một danh sách các dst (kiểu list) được chuyển đổi từ chuỗi dst_line. \n","Các giá trị trong danh sách dst được chuyển đổi thành kiểu int và được lọc bỏ các khoảng trắng không cần thiết.'''\n","def line2dataset(line):\n","    src, dst_line= line.split('\\t')\n","    src = int(src.strip())\n","    dst_list = [int(x.strip()) for x in dst_line.split(',') if x != '']\n","    return src, dst_list\n","\n","'''Hàm filter_pairs nhận vào một cặp dữ liệu x là một tuple chứa hai phần tử là các tuple khác: (a, [b1, b2, ..., bn]) và (c, [d1, d2, ..., dm]). \n","Hàm này sẽ kiểm tra xem có thể tạo thành một cặp (a, [c, shared]) mới không, trong đó shared là số lượng phần tử chung giữa danh sách b và danh sách d.'''\n","def filter_pairs(x):\n","    if (x[0][0] != x[1][0]) and (not x[0][0] in x[1][1]) and (not x[1][0] in x[0][1]):\n","        shared = len(list(set(x[0][1]).intersection(set(x[1][1]))))\n","        return (x[0][0],[x[1][0],shared])\n","\n","'''Hàm map_finaldataset nhận vào một phần tử dữ liệu elem có định dạng (src, [dst, shared]) \n","và trả về một tuple mới có định dạng (src, recommendations), trong đó recommendations là danh sách 10 nút được đề xuất cho src. \n","Danh sách này được tạo ra bằng cách sắp xếp các nút trong dst theo thứ tự giảm dần của số lượng nút chung shared và theo thứ tự tăng dần của giá trị dst.'''\n","def map_finaldataset(elem):\n","    src = elem[0]\n","    dst_commons = elem[1]\n","    dst_commons=sorted(dst_commons,key=lambda x:(-x[1],x[0]))[:10]\n","    recommendations=[pair[0] for pair in dst_commons]\n","    return (src, recommendations)"],"metadata":{"id":"L6_FYmkhG7QY"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dn3oJzX23EsA","outputId":"fca41695-7bf1-4ae5-bf27-be8cb61664a8"},"source":["dataset = dataset.map(line2dataset)\n","\n","check_users = [924, 8941, 8942, 9019, 9020, 9021, 9022, 9990, 9992, 9993]\n","cartesian = dataset.cartesian(dataset).filter(lambda x: x[0][0] in check_users)\n","\n","'''Dòng lệnh dưới thực hiện tính toán và tạo ra danh sách đề xuất 10 nút cho mỗi src trong RDD cartesian.'''\n","dataset = cartesian.map(filter_pairs).filter(lambda x: x != None and x[1][1] > 0)\\\n","    .filter(lambda x: x[0] in check_users) \\\n","    .groupByKey().mapValues(list).map(map_finaldataset)\n","\n","id_check_dataset = dataset.filter(lambda x: x[0] in check_users).collect()\n","\n","for key, val in id_check_dataset:\n","    print('id:', key,' recommendations:', val)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["id: 9020  recommendations: [9021, 9016, 9017, 9022, 317, 9023]\n","id: 924  recommendations: [439, 2409, 6995, 11860, 15416, 43748, 45881]\n","id: 9992  recommendations: [9987, 9989, 35667, 9991]\n","id: 9021  recommendations: [9020, 9016, 9017, 9022, 317, 9023]\n","id: 9993  recommendations: [9991, 13134, 13478, 13877, 34299, 34485, 34642, 37941]\n","id: 8941  recommendations: [8943, 8944, 8940]\n","id: 9022  recommendations: [9019, 9020, 9021, 317, 9016, 9017, 9023]\n","id: 9990  recommendations: [13134, 13478, 13877, 34299, 34485, 34642, 37941]\n","id: 8942  recommendations: [8939, 8940, 8943, 8944]\n","id: 9019  recommendations: [9022, 317, 9023]\n"]}]}]}